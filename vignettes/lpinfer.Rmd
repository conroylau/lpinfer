---
title: 'lpinfer: An R Package for Inference in Linear Programs'
author: ''
output:
  md_document:
    preserve_yaml: yes
    toc: yes
    df_print: paged  
  pdf_document:
    toc: yes
    df_print: paged
  github_document:
    keep_html: yes
    toc: yes
    df_print: paged
  html_document:
    keep_md: yes
    toc: yes
    df_print: paged
bibliography: refs.bib
vignette: |
  %\VignetteIndexEntry{lpinfer} 
  %\VignetteEncoding{UTF-8} 
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup1, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE
)
```

```{r setup2, include = FALSE}
require(lpinfer)
require(data.table)
require(pander)
require(gurobi)
require(knitr)
require(kableExtra)
require(foreach)
require(doMC)
require(parallel)
require(PtProcess)
require(doSNOW)
require(doRNG)
require(doParallel)
require(future)
RNGkind(kind = "L'Ecuyer-CMRG")
```

## Introduction
This package provides a set of methods to conduct inference on econometrics 
problems that can be studied by linear programs. Currently, this package 
supports the following tests:

1. Cone-tightening procedure by @dkqs2018
1. Subsampling procedure
1. FSST procedure by @fsst2020
1. Procedure by @cr2019

Apart from computing the *p*-values based on the above tests, this package
can also construct confidence intervals and estimate the bounds of the 
estimators in linear programs subject to certain shape constraints.

## Scope of the Vignette
This vignette is intended as a guide to use the `lpinfer` package without
further explanation for the methods. Readers may refer to each of the 
sections for the tests for references to more details about the 
methods.

## Installation and Requirements
`lpinfer` can be installed from our GitHub repository via

```{r install.github, eval = FALSE}
devtools::install_github("conroylau/lpinfer")
```

To use most of the functions in this `lpinfer` package, one of the following
packages for solving the linear and quadratic programs is required. There are 
four options for the solver:

1. Gurobi and the R package `gurobi` --- Gurobi can be downloaded from
[Gurobi Optimization](https://www.gurobi.com/). A Gurobi software license is 
required. The license can be obtained at no cost for academic researchers. The 
instructions for installing `gurobi` on R can be found 
[here](https://cran.r-project.org/web/packages/prioritizr/vignettes/gurobi_installation.html#r-package-installation).

1. IBM ILOG CPLEX Optimization Studio (CPLEX) and one of the R packages below 
--- CPLEX can be downloaded from 
[IBM](https://www.ibm.com/analytics/cplex-optimizer). A CPLEX software license
is required, which can be obtained at no cost for academic researchers. There 
are two open-source and free R packages that uses CPLEX, and users are free to
choose one of them. In addition, both packages have to be installed on the 
command line to link the package to the correct CPLEX library. The two packages' 
name and installation instructions are as follows:
   i.  `Rcplex`  --- the instructions to install the R package can be found
   [here](https://cran.r-project.org/web/packages/Rcplex/INSTALL).
   
   i. `cplexAPI` --- the instructions to install the R package can be found
   [here](https://cran.r-project.org/web/packages/cplexAPI/INSTALL).

1. `limSolve` --- a free and open-source package available on CRAN. This can be
installed directly via the `install.packages` command in R.

If no package is specified, one of the above packages will be automatically 
chosen from those that are available.

The package `lpSolveAPI` is only supported in the function `estbounds`
when the L1-norm is used. This is a free and open-source package available on
CRAN. This can be installed directly via the `install.packages` command in R.

## Sample data
The classical missing data problem due to @manski1989 is used as a running
example throughout this vignette to demonstrate the commands in this 
`lpinfer` package. We consider this problem here because the sharp bounds for 
the identified set of the expected values can be constructed by linear programs.

In this package, a sample simulated data set on the missing data problem is
included. This can be obtained by `sampledata`. This data set contains 1,000
observations with 2 columns. The following shows the first 10 observations of 
the simulated data set:

```{r drawData-df, eval = TRUE}
library(lpinfer)
knitr::kable(head(sampledata, n = 10))
```
where

* `Y` is a multivariate discrete outcome variable that takes value from 0 to 1 
with increment 0.1.
* `D` is a binary treatment variable where *Y*~*i*~ is observed for 
*D*~*i*~ = 1 and not observed for *D*~*i*~ = 0.

## General Syntax

In general, each of the tests in the `lpinfer` requires a data set, a 
`lpmodel` object and some tuning parameters. The tuning parameters are 
different for each of the test and will be explained in later sections. 
For the `lpmodel` object, it consists of five components:

* `A.obs`
* `A.shp`
* `A.tgt`
* `beta.obs`
* `beta.shp`

Here, `A.obs` and `beta.obs` refers to the matrix of coefficients and the RHS
variables for the "observed constraints" in the linear program. `A.shp` and
`beta.shp` refers to the matrix of coefficients and the RHS variables for
the shape constraints. `A.tgt` refers to the matrix of coefficients for the 
linear constraints that we wish to test with `beta.tgt`. Since the object 
`beta.tgt` is a parameter that is being tested, it will be defined outside
the `lpmodel` object. 

Note that not all procedures will require all five components to be present
in the `lpmodel` object. For instance, `A.shp` and `beta.shp` are not required
in the `dkqs` procedure. 

#### Standard form

For consistency, the tests in this `lpinfer` package assumes that all of the 
components in the `lpmodel` object represent constraints that are  
in standard form. Hence, the matrices in the `lpmodel` object are representing 
equality constraints. To impose inequality constraints, users need to first 
convert them to standard form by adding appropriate slack and surplus variables.

This can be done easily with the `standard.lpmodel` function in this `lpinfer`
package with an object in the `lpmodel.natural` class. The `lpmodel.natural` 
class consists of the following eight components:

* `A.obs`
* `A.shp`
* `A.tgt`
* `beta.obs`
* `beta.shp`
* `sense.shp`
* `x.lb`
* `x.ub`

The first five components are the same as the `lpmodel` class. The `sense.shp`
vector stores the sense of the shape constraints (i.e. `>=`, `=` or `<=`). 
The two components `x.lb` and `x.ub` correspond to the lower and upper
bound of the `x` vector respectively.

The procedure to convert an object from the `lpmodel.natural` class into the
`lpmodel` class is as follows:

1. Create an object in the `lpmodel.natural` class.
2. Apply the `standard.lpmodel` function on this object.

Below is an example of using the `standard.lpmodel` function on the object
in the `lpmodel.natural` class, which is denoted by `lpmn0` in this example.
```{r standard.form1}
### Step 1: Create an object in the `lpmodel.natural` class
# Obs
Aobs0 <- matrix(c(1, 2), nrow = 1)
bobs0 <- c(10)

# Shp
Ashp0 <- matrix(c(3, 4, 5, 6), nrow = 2, byrow = TRUE)
bshp0 <- matrix(c(15, 100))
sshp0 <- matrix(c(">=", "<="))

# Tgt
Atgt0 <- matrix(c(1, 1), nrow = 1)

# Upper bounds
xub0 <- c(200, 200)

# Lower bounds
xlb0 <- c(0.1, 0.1)

# Formulate the `lpmodel.natural` object
lpmn0 <- lpmodel.natural(A.obs = Aobs0,
                         A.shp = Ashp0,
                         A.tgt = Atgt0,
                         beta.obs = bobs0,
                         beta.shp = bshp0,
                         sense.shp = sshp0,
                         x.ub = xub0,
                         x.lb = xlb0)
```

Then, these constraints can be transformed into a `lpmodel` object in standard
form as follows:
```{r standard.form2}
### Step 2: Apply the `standard.lpmodel` function
lpm1 <- standard.lpmodel(lpmn0)
```

The updated `A.shp` and `beta.shp` matrices reflect the updated shape 
constraints and the lower and upper bounds in standard form by incorporating 
slack and surplus variables. The two matrices can be viewed as follows:
```{r standard.form3a}
print(lpm1$A.shp)
print(lpm1$beta.shp)
```
- The first row corresponds to the `<=` constraint in the original `Ashp0` 
and `bshp0` matrix.
- The second and third rows correspond to the upper bounds.
- The fourth row corresponds to the `>=` constraint in the original `Ashp0` 
and `bshp0` matrix.
- The last two rows correspond to the lower bounds.

The resulting object `lpm1` is already in standard form and can be passed
to the tests in this `lpinfer` package.

#### Deterministic or stochastic components

Depending on the testing procedure, the five components in the `lpmodel` 
object can be either deterministic or stochastic. The component is said to 
be deterministic if it remains unchanged in the bootstrap procedure. The
component is said to be stochastic if it changes (and depends on the bootstrap 
data) in the bootstrap procedure.

* If the component is deterministic, then the component is typically a 
`matrix` or a `data.frame`. 

* If the component is stochastic, then it is represented by a
`function` or a `list`. If the component is a `function`, then it will be 
re-evaluated based on the bootstrap data in each bootstrap draw. 
If the component is a `list`, then each object in the list represents 
the component in the bootstrap draw.

* If `data` is passed to the testing procedure, then the argument `n` (that
represents the total number of observations in the `data` is optional).
Otherwise, when `data` is not passed to the testing procedure (which
typically refers to the case where the bootstrap estimates have already been
incorporated in the `lpmodel` object), then the argument `n` is required.

#### Remarks for the component being a function

If the component is a `function`, then it has to fulfill the following 
requirements:

* The function's only argument is the data set. The function needs to accept
data sets in the class `data.frame`.
* The function can return either one or two objects. If it returns one object, 
then it has to be a vector or a column matrix that represents the estimator for
`beta.obs`. If it has two objects, then it returns the estimator
for `beta.obs` and a square matrix that refers to the estimator of the 
asymptotic variance of `beta.obs`.

#### Demonstration {#demonstration}

The following is an example on how to construct each of the components in the
`lpmodel` object and how to assign the `lpmodel` object based on the example 
data mentioned [here](#sample-data):


```{r syntax.fullinfo, eval = TRUE}
# Extract relevant information from data
N <- nrow(sampledata)
J <- length(unique(sampledata[,"Y"])) - 1
J1 <- J + 1

# Construct A.obs
Aobs.full <- cbind(matrix(rep(0, J1 * J1), nrow = J1), diag(1, J1))

# Construct A.tgt
yp <- seq(0, 1, 1/J)
Atgt <- matrix(c(yp, yp), nrow = 1)

# Construct A.shp
Ashp <- matrix(rep(1, ncol(Aobs.full)), nrow = 1)

# Construct beta.obs (a function)
betaobs.fullinfo <- function(data) {
  beta <- NULL
  y.list <- sort(unique(data[,"Y"]))
  n <- dim(data)[1]
  yn <- length(y.list)
  for (i in 1:yn) {
    beta.i <- sum((data[,"Y"] == y.list[i]) * (data[,"D"] == 1))/n
    beta <- c(beta, c(beta.i))
  }
  beta <- as.matrix(beta)
  return(list(beta = beta,
              var = diag(yn)))
}

# Define the lpmodel object
lpm.full <- lpmodel(A.obs    = Aobs.full,
                    A.tgt    = Atgt,
                    A.shp    = Ashp,
                    beta.obs = betaobs.fullinfo,
                    beta.shp = c(1))

```

In the above, the `betaobs.fullinfo` function returns two information: `beta`
is a vector of length that is equal to the number of distinct observations 
for *Y* where element *i* of the vector refers to the probability that the
corresponding value of *y*~*i*~ is observed. The `var` object is the 
estimator of the asymptotic variance of `beta`, which is assumed to be an
identity matrix here for illustration purpose.

## Tests

### Test 1: DKQS cone-tightening procedure

The `dkqs` function in this `lpinfer` package is used to conduct the 
cone-tightening procedure that is proposed by @dkqs2018. For details of the
procedure, readers may refer to section 4.2 and the appendix of @dkqs2018. 

#### Syntax
The `dkqs` command has the following syntax:
```{r syntax.dkqs , eval = FALSE}
dkqs(data = sampledata,
     lpmodel = lpm.full,
     beta.tgt = .375,
     R = 100,
     tau = sqrt(log(N)/N),
     n = NULL,
     solver = "gurobi",
     progress = FALSE)
```

where

* `data` refers to the data set.
* `lpmodel` refers to the `lpmodel` object.
* `beta.tgt` refers to the parameter that is being tested.
* `R` refers to the total number of bootstraps.
* `tau` refers to the tuning parameter tau. This will be explained 
[here](#choosing-the-tau-parameter). It can be a scalar or a vector.
* `n` refers to the total number of observations in `data`. This is optional 
if `data` is passed to the testing procedure. See
[here](#deterministic-or-stochastic-components) for more details.
* `solver` refers to the name of the solver.
* `progress` refers to the boolean variable for whether the progress bar
should be printed in the testing procedure.

#### Choosing the tau parameter {#choosing-the-tau-parameter}

Depending on the value of `tau`, the main quadratic program in @dkqs2018 is 
not always feasible. The details on how to choose the maximum tau such 
that the `dkqs` procedure is feasible can be found in the supplemental
appendix of @kamat2019.

In this module, we follow the procedure by @kamat2019 to pick the largest 
feasible `tau`. If the value of `tau` chosen by the user is infeasible, then
the module will evaluate the procedure using the largest possible `tau`. 
Otherwise, the module will evaluate the procedure with the given `tau`.

If `tau` is not specified by the user, the procedure will directly use
the maximum feasible `tau`.

On the other hand, this package provides the flexibility to the users to 
pass multiple tuning parameters at the same time. Hence, `tau` can be a 
vector as well.

#### Components in `lpmodel`

The following table summarizes whether the components in `lpmodel`
can be deterministic or stochastic:

| Component in `lpmodel`| Property |
| --------------- | :-------------- |
|`A.obs`| Deterministic |
|`A.shp`| Not used |
|`A.tgt`| Deterministic |
|`beta.obs`| Stochastic |
|`beta.shp`| Not used |

`A.shp` and `beta.shp` will be ignored in the `dkqs` procedure.

#### Data structure

The function defined in this function needs to generate an estimator
for the `beta.obs` object. 

In addition, this procedure has an additional requirement on the data 
structure: the column that represents the outcome variable has to be called
either "Y" or "y".

#### Example

The following is what happens when the above code is run:
``` {r dkqs.eg1, eval = TRUE}
set.seed(1)
dkqs.full1 <- dkqs(data = sampledata,
                   lpmodel = lpm.full,
                   beta.tgt = .375,
                   R = 100,
                   tau = sqrt(log(N)/N),
                   solver = "gurobi",
                   progress = FALSE)
print(dkqs.full1)
```

As noted in the syntax section, we provide the flexibility for users to conduct
inference with multiple tuning parameters at the same time. The following is
an example when the user specifies multiple taus in the `dkqs` procedure:
``` {r dkqs.eg2, eval = TRUE}
set.seed(1)
dkqs.full2 <- dkqs(data = sampledata,
                   lpmodel = lpm.full,
                   beta.tgt = .375,
                   R = 100,
                   tau = c(.1, .2, .3, .5),
                   solver = "gurobi",
                   progress = FALSE)
print(dkqs.full2)
```

Users can get a more detailed summary of the results when applying the 
`summary` command on the resulting object:
``` {r dkqs.eg2summary, eval = TRUE}
summary(dkqs.full2)
```

#### Alternative approach
The approach that has been demonstrated so far is known as the 
full information approach. The inference procedures in this `lpinfer` package
is not limited to any single approach. In this section, we demonstrate 
how to use the two moments approach. This alternative method is known as the
two moments approach because the two moments **E**[*Y*~*i*~] and 
**E**[*Y*~*i*~*D*~*i*~] are used in the inference procedure.

Similar to what has been demonstrated in the [example](#demonstration) earlier,
some of the components has to be updated in the `lpmodel` object. 
This can be done as follows:

```{r syntax.twom, eval = TRUE}
# Construct A.obs
Aobs.twom <- matrix(c(rep(0,J1), yp, rep(0,J1), rep(1, J1)), nrow = 2,
                     byrow = TRUE)

# Construct beta.obs (a function)
betaobs.twom <- function(data) {
  beta <- matrix(c(0,0), nrow = 2)
  n <- dim(data)[1]
  beta[1] <- sum(data[,"Y"] * data[,"D"])/n
  beta[2] <- sum(data[,"D"])/n
  return(list(beta = beta,
              var = diag(2)))
}

# Define the lpmodel object
lpm.twom <- lpmodel(A.obs    = Aobs.twom,
                    A.tgt    = Atgt,
                    A.shp    = Ashp,
                    beta.obs = betaobs.twom,
                    beta.shp = c(1))

```

The `dkqs` procedure can be ran in the same fashion as before. The following
is an example on how this can be ran:

``` {r dkqs.eg3, eval = TRUE}
set.seed(1)
dkqs.full3 <- dkqs(data = sampledata,
                   lpmodel = lpm.twom,
                   beta.tgt = .375,
                   R = 100,
                   tau =  sqrt(log(N)/N),
                   solver = "gurobi",
                   progress = FALSE)
print(dkqs.full3)
```

As shown above, we get the same *p*-value as before.

### Test 2: Subsampling procedure

The `subsample` function in this package carries out the test using 
the subsampling procedure. 

#### Syntax

The `subsample` command has the following syntax:
```{r subsample.syntax, eval = FALSE}
subsample(data = sampledata, 
          lpmodel = lpm.full,
          beta.tgt = 0.375,
          R = 100,
          solver = "gurobi",
          norm = 2,
          phi = 2/3,
          n = NULL,
          replace = FALSE,
          progress = FALSE)
```
where 

* `phi` refers to the parameter that controls the size of each subsample. 
This will be further explained [here](#choosing-the-phi-and-replace-parameter).
* `replace` refers to the boolean variable to indicate whether the function 
samples the data with or without replacement. This will be further explained
[here](#choosing-the-phi-and-replace-parameter).
* `norm` refers to the norm used in the objective function.

The rest of the arguments are the same as that in the `dkqs` procedure.

#### Choosing the `phi` and `replace` parameter

The `phi` parameter is a parameter to control the size of each subsample. 
The sample size of the original data to the power `phi` is the size of each 
subsample. On the other hand, the `replace` parameter is used to indicate 
whether the function samples the data with or without replacement. 

| `replace`| `phi` | Meaning |
| --------------- | :-------------- | :-------------- |
|`FALSE`| Any number in the interval (0, 1) | Subsample |
|`TRUE`| Equals to 1 | Bootstrap |
|`TRUE`| Any number in the interval (0, 1) | *m* out of *n* bootstrap |

Note that users cannot specify `phi` as 1 when `replace` is set to `FALSE`
because it will be generating the exactly same set of data in every 
subsample draw.

#### Components in `lpmodel`

The following table summarizes whether the components in `lpmodel`
can be deterministic or stochastic:

| Component in `lpmodel`| Property |
| --------------- | :-------------- |
|`A.obs`| Deterministic or stochastic |
|`A.shp`| Deterministic or stochastic |
|`A.tgt`| Deterministic or stochastic |
|`beta.obs`| Stochastic |
|`beta.shp`| Deterministic or stochastic |

#### Example

The following is what happens when the above code is run:
``` {r subsample.eg1, eval = TRUE}
set.seed(1)
subsample.full <- subsample(data = sampledata, 
                            lpmodel = lpm.full,
                            beta.tgt = 0.375,
                            R = 100,
                            solver = "gurobi",
                            norm = 2,
                            phi = 2/3,
                            replace = FALSE,
                            progress = FALSE)
print(subsample.full)
```

Again, more detailed information can be extracted via the `summary` command:
``` {r subsample.eg2, eval = TRUE}
summary(subsample.full)
```

As indicated [earlier](#choosing-the-phi-and-replace-parameter), the 
`subsample` command can perform the bootstrap and *m* out of *n* bootstrap 
procedures. They are illustrated as follows.

The following is an example of performing a bootstrapping procedure:

``` {r subsample.eg3, eval = TRUE}
set.seed(1)
subsample.bootstrap <- subsample(data = sampledata, 
                                 lpmodel = lpm.full,
                                 beta.tgt = 0.375,
                                 R = 100,
                                 solver = "gurobi",
                                 norm = 2,
                                 phi = 1,
                                 replace = TRUE,
                                 progress = FALSE)
print(subsample.bootstrap)
```

The following is an example of performing a *m* out of *n* bootstrapping 
procedure:

``` {r subsample.eg4, eval = TRUE}
set.seed(1)
subsample.bootstrap2 <- subsample(data = sampledata, 
                                  lpmodel = lpm.full,
                                  beta.tgt = 0.375,
                                  R = 100,
                                  solver = "gurobi",
                                  norm = 2,
                                  phi = 2/3,
                                  replace = TRUE,
                                  progress = FALSE)
print(subsample.bootstrap2)
```

### Test 3: FSST procedure

The `fsst` function in this `lpinfer` package is used to conduct the 
testing procedure by @fsst2020.

#### Syntax

The `fsst` command has the following syntax:
```{r fsst.syntax, eval = FALSE}
fsst(data = sampledata, 
     lpmodel = lpm.full,
     beta.tgt = 0.375,
     R = 100,
     lambda = 0.5,
     rho = 1e-4,
     n = NULL,
     weight.matrix = "diag",
     solver = "gurobi",
     progress = FALSE)
```
where 

* `lambda` refers to the tuning parameter that is used to obtain the bootstrap
estimates of the cone component in the test statistics. Users can pass 
multiple `lambda`s in this argument. In addition, we also provide a 
data-driven approach to choose the `lambda` parameter. The details can be found 
[here](#data-driven-lambda).
* `rho` refers to the parameter used to studentize the variance matrices
in the FSST procedure.
* `weight.matrix` is a string that determines the weighting matrix. 
The details can be found [here](#weighting-matrix).

The rest of the arguments are the same as that in the `dkqs` procedure.

#### Weighting matrix

This procedure provides three options for the weighting matrix in the FSST
procedure:

|`weight.matrix` | Definition |
| --------------- | :-------------- |
|`identity`| Identity matrix|
|`avar`| Inverse of the asymptotic variance of `beta.obs`|
|`diag`| Diagonal matrix with elements equal to the diagonal entries of `avar`|

#### Components in `lpmodel`

The following table summarizes whether the components in `lpmodel`
can be deterministic or stochastic:

| Component in `lpmodel`| Property |
| --------------- | :-------------- |
|`A.obs`| Deterministic |
|`A.shp`| Deterministic |
|`A.tgt`| Deterministic |
|`beta.obs`| Stochastic |
|`beta.shp`| Deterministic |

#### Example

The following is what happens when the above code is run:
``` {r fsst.eg1, eval = TRUE}
set.seed(1)
fsst.full1 <- fsst(data = sampledata, 
                   lpmodel = lpm.full,
                   beta.tgt = 0.375,
                   R = 100,
                   lambda = 0.5,
                   rho = 1e-4,
                   weight.matrix = "diag",
                   solver = "gurobi",
                   progress = FALSE)
print(fsst.full1)
```

As noted above, the `fsst` procedure provides the flexibility for users to 
pass multiple tuning parameters.

``` {r fsst.eg2, eval = TRUE}
set.seed(1)
fsst.full2 <- fsst(data = sampledata, 
                   lpmodel = lpm.full,
                   beta.tgt = 0.375,
                   R = 100,
                   lambda = c(0.1, 0.2, 0.5),
                   rho = 1e-4,
                   weight.matrix = "diag",
                   solver = "gurobi",
                   progress = FALSE)
print(fsst.full2)
```

Again, more detailed information can be extracted via the `summary` command:
``` {r fss.eg2summary, eval = TRUE}
summary(fsst.full2)
```

#### Data-driven `lambda`
To use the data-driven `lambda` in the FSST procedure, users can set `lambda` 
as `NA` or include `NA` as one of the elements in the `lambda` vector. For
instance, if `lambda` is  set as `c(0.1, NA)`, then both 0.1 and the 
data-driven `lambda` will be applied in the FSST procedure. 

For instance, the following code uses 0.1 and the data-driven `lambda` to
compute the *p*-value:
``` {r fsst.eg3, eval = TRUE}
set.seed(1)
fsst.full3 <- fsst(data = sampledata, 
                   lpmodel = lpm.full,
                   beta.tgt = 0.375,
                   R = 100,
                   lambda = c(0.1, NA),
                   rho = 1e-4,
                   weight.matrix = "diag",
                   solver = "gurobi",
                   progress = FALSE)
print(fsst.full3)
```

In the FSST procedure, the default is to use the data-driven `lambda`.

### Test 4: Cho-Russell procedure

The `chorussell` function in this `lpinfer` package is used to conduct the 
testing procedure by @cr2019.

#### Syntax

The `chorussell` command has the following syntax:
```{r cr.syntax, eval = FALSE}
chorussell(data = sampledata, 
           lpmodel = lpm.full,
           beta.tgt = 0.375,
           R = 100,
           kappa = 1e-5,
           norm = 2,
           n = NULL,
           estimate = TRUE,
           solver = "gurobi",
           ci = FALSE,
           alpha = 0.05,
           tol = 1e-4,
           progress = FALSE)
```
where 

* `ci` refers to whether a confidence interval or the *p*-value is returned.
By default, the *p*-value is returned.
* `alpha` refers to the significance level. This argument is not required if
`ci` is set as `FALSE`.
* `tol` refers to the tolerance level used in the bisection method to search
for the *p*-value. This argument is not required if `ci` is set as `TRUE`.

The rest of the arguments are the same as that in the `dkqs` and the 
`estbounds` procedures. In this function, `kappa` can be a vector.

#### Components in `lpmodel`

The following table summarizes whether the components in `lpmodel`
can be deterministic or stochastic:

| Component in `lpmodel`| Property |
| --------------- | :-------------- |
|`A.obs`| Deterministic |
|`A.shp`| Deterministic |
|`A.tgt`| Deterministic |
|`beta.obs`| Stochastic |
|`beta.shp`| Deterministic |

#### Example (Returning confidence interval)

To construct a confidence interval, the option `ci` has to be specified as
`TRUE`. The following code can be used to construct a 95%-confidence interval:
```{r cr.eg1, eval = TRUE}
set.seed(1)
cr.ci1 <- chorussell(data = sampledata, 
                     lpmodel = lpm.full,
                     beta.tgt = 0.375,
                     R = 100,
                     kappa = 1e-5,
                     norm = 2,
                     estimate = TRUE,
                     solver = "gurobi",
                     ci = TRUE,
                     alpha = 0.05,
                     progress = FALSE)
print(cr.ci1)
```

As noted above, the `kappa` parameter can be a vector so one confidence 
interval will be returned for each `kappa`:
```{r cr.eg2, eval = TRUE}
set.seed(1)
cr.ci2 <- chorussell(data = sampledata, 
                     lpmodel = lpm.full,
                     beta.tgt = 0.375,
                     R = 100,
                     kappa = c(1e-3, 1e-5),
                     norm = 2,
                     estimate = TRUE,
                     solver = "gurobi",
                     ci = TRUE,
                     alpha = 0.05,
                     progress = FALSE)
print(cr.ci2)
```

Again, more detailed information can be obtained via the `summary` command:
```{r cr.eg3, eval = TRUE}
summary(cr.ci2)
```

#### Example (Returning *p*-value)

To compute the *p*-value, the option `ci` has to be specified as `FALSE`.
The following code can be run to compute the *p*-value with tolerance level
being 0.001:
```{r cr.eg4, eval = TRUE}
set.seed(1)
cr.pv1 <- chorussell(data = sampledata, 
                     lpmodel = lpm.full,
                     beta.tgt = 0.375,
                     R = 100,
                     kappa = 1e-5,
                     norm = 2,
                     estimate = TRUE,
                     solver = "gurobi",
                     ci = FALSE,
                     tol = 0.001,
                     progress = FALSE)
print(cr.pv1)
```

Again, the `kappa` parameter can be a vector:
```{r cr.eg5, eval = TRUE}
set.seed(1)
cr.pv2 <- chorussell(data = sampledata, 
                     lpmodel = lpm.full,
                     beta.tgt = 0.375,
                     R = 100,
                     kappa = c(1e-3, 1e-5),
                     norm = 2,
                     estimate = TRUE,
                     solver = "gurobi",
                     ci = FALSE,
                     tol = 0.001,
                     progress = FALSE)
print(cr.pv2)
```

More detailed information can be obtained via the `summary` command:
```{r cr.eg6, eval = TRUE}
summary(cr.pv2)
```


## Constructing Bounds subject to Shape Constraints

Another key application of the `lpinfer` package is to construct the 
bounds estimates subject to certain shape constraints. This is obtained by the
`estbounds` function. The linear program for obtaining the exact bounds 
subject to shape constraints may not be necessarily feasible. Hence, an 
`estimate` option is available. The estimation can be conducted via L1-norm 
or L2-norm.

#### Syntax

``` {r estbounds.syntax, eval = FALSE}
estbounds(data = sampledata,
          lpmodel = lpm.full,
          kappa = 1e-5,
          norm = 2,
          solver = "gurobi",
          estimate = TRUE,
          progress = FALSE)
```
where 

* `norm` refers to the norm used in the optimization problem. The norms
that are supported by this function are L1-norm and L2-norm. See 
[here](#norms) for more details.
* `kappa`  refers to the parameter used in the second step of the two-step
procedure for obtaining the solution subject to the shape constraints.
* `estimate` refers to the boolean variable that indicate whether the estimated
problem should be considered.

The rest of the arguments are the same as that in the `dkqs` procedure.

#### Norms
In constructing the estimated bounds, users are free to choose the L1-norm or
the L2-norm. For the estimation with L2-norm, users need to choose `gurobi` as 
the solver. For the estimation with L1-norm, users can choose one of the 
following packages as the solver:

* `gurobi`,
* `limSolve`,
* `cplexAPI`,
* `Rcplex`,
* `lpSolveAPI`.


#### Example

The following is what happens when the above code is run:
``` {r estbounds.eg1, eval = TRUE}
set.seed(1)
estbounds.full <- estbounds(data = sampledata,
                            lpmodel = lpm.full,
                            kappa = 1e-5,
                            norm = 2,
                            solver = "gurobi",
                            estimate = TRUE,
                            progress = FALSE)
print(estbounds.full)
```

Again, more detailed information can be extracted via the `summary` command:
``` {r estbounds.eg1summary, eval = TRUE}
summary(estbounds.full)
```


## Constructing Confidence Intervals

Apart from conducting inference and estimating the bounds, the `lpinfer` 
package can also construct confidence intervals via the `invertci` function. 
The confidence interval is constructed by evaluating the *p*-value of a test 
and applying the bisection method.

#### Syntax {#invertci_syntax}
The syntax of the `invertci` function is as follows:

```{r invertci.syntax, eval = FALSE}
invertci(f = subsample, 
         farg = subsample.args, 
         alpha = 0.05, 
         lb0 = NULL, 
         lb1 = NULL, 
         ub0 = NULL, 
         ub1 = NULL, 
         tol = 0.0001, 
         max.iter = 20, 
         df_ci = NULL,
         progress = FALSE)
```
where 

* `f` refers to the function that represents a testing procedure.
* `farg` refers to the list of arguments to be passed to the function of 
testing procedure. The details can be found [here](#specifying-the-argument).
* `alpha` refers to the significance level of the test. Please refer to the 
details [here](#constructing-multiple-confidence-intervals).
* `lb0` refers to the logical lower bound for the confidence interval.
* `lb1` refers to the maximum possible lower bound for the confidence interval.
* `ub0` refers to the logical upper bound for the confidence interval.
* `ub1` refers to the minimum possible upper bound for the confidence interval.
* `tol` refers to the tolerance level in the bisection method.
* `max.iter` refers to the maximum number of iterations in the bisection 
method.
* `df_ci` refers to `data.frame` that consists of the points and the 
corresponding *p*-values that have been tested in constructing the confidence 
intervals. The details can be found 
[here](#specifying-the-data-frame-df_ci).
* `progress` refers to the boolean variable for whether the result messages 
should be displayed in the procedure of constructing confidence interval. 

#### Specifying the argument
To use the `invertci` function, the arguments for the test statistic has to
be specified and passed to the `farg` argument. For instance, if the testing
procedure `subsample` is used, the arguments can be defined as follows:

```{r invertci.args, eval = TRUE}
subsample.args <- list(data = sampledata,
                       lpmodel = lpm.full,
                       R = 100,
                       phi = .75,
                       solver = "gurobi",
                       progress = FALSE)
```

Note that the argument for the target value of beta, i.e. the value to be 
tested under the null, is not required in the above argument assignment.

#### Specifying the Data Frame `df_ci` {#dfci_invertci}
If the *p*-values at certain points have already been evaluated, users can
store them in a data frame and pass it to the function `invertci`. The 
requirement for the data frame is as follows:

* The data frame can only has two columns. The first column is `point` (which
contains the values of betas that has been evaluated) and the second column is
`value` (which corresponds to the *p*-values being evaluated).
* The data frame can only contain numeric values.

#### Example

The following shows a sample output of the function `invertci` that is used to 
the confidence interval for the test `dkqs` with significance level 0.05.

```{r invertci.eg1, eval = TRUE}
set.seed(1)
invertci.subsample1 <- invertci(f = subsample,
                                farg = subsample.args, 
                                alpha = 0.05, 
                                lb0 = 0, 
                                lb1 = 0.4, 
                                ub0 = 1, 
                                ub1 = 0.6, 
                                tol = 0.001, 
                                max.iter = 5, 
                                df_ci = NULL, 
                                progress = FALSE)
print(invertci.subsample1)
```

The details for each iteration can be obtained in real-time by setting
`progress` as `TRUE` or applying the `summary` command on the resulting
object:
``` {r invertci.summary1, eval = TRUE}
summary(invertci.subsample1)
```

#### Constructing Multiple Confidence Intervals

The function `invertci` can also be used to generate multiple confidence
intervals if the argument `alpha` is a vector. For instance, the following
code produces confidence intervals for alpha equals 0.01, 0.05 and 0.1.

```{r invertci.eg2, eval = TRUE}
set.seed(1)
invertci.subsample2 <- invertci(f = subsample, 
                                farg = subsample.args, 
                                alpha = c(0.01, 0.05, 0.1), 
                                lb0 = 0, 
                                lb1 = 0.4, 
                                ub0 = 1, 
                                ub1 = 0.6, 
                                tol = 0.001, 
                                max.iter = 5, 
                                df_ci = NULL, 
                                progress = FALSE)
print(invertci.subsample2)
```

Again, the detailed steps in constructing the confidence intervals can be 
obtained as follows:
``` {r invertci.summary2, eval = TRUE}
summary(invertci.subsample2)
```

Since the details of the iterations could potentially be a long list of
outputs, users may specify a particular list of output that they wish to
read in the `summary` command. For instance, to only print the details of the 
iterations when the significance level is 0.05, it can be done as follows:
``` {r invertci.summary2b, eval = TRUE}
summary(invertci.subsample2, alphas = 0.05)
```

## Parallel Programming

The `lpinfer` package supports parallel programming through the `future` 
package.  The `future` package is freely available on CRAN and can be installed 
directly via the `install.packages` command in R.

To implement parallel programming, users need to first install and load the
`future` package. Then, specify the number of workers to be used via the
`plan` function before running any functions from the `lpinfer` package
via the `plan` function after loading the `future` package. 

Below is an example of running the `dkqs` function with parallel programming
by the `future` package:
``` {r dkqs.eg.cores, eval = FALSE}
library(future)
plan(multisession, workers = 8)
set.seed(1)
dkqs.full1 <- dkqs(data = sampledata,
                   lpmodel = lpm.full,
                   beta.tgt = .375,
                   R = 1000,
                   tau = sqrt(log(N)/N),
                   solver = "gurobi",
                   progress = FALSE)
print(dkqs.full1)
```

The computational time for using multiple cores should be
shorter than using a single core for a large number of bootstraps. This is
illustrated by the example via the `dkqs` procedure below:

```{r dkqs.eg.cores2, eval = TRUE}
library(future)
dkqs.args <- list(data = sampledata,
                  lpmodel = lpm.full,
                  beta.tgt = .375,
                  R = 5000,
                  tau = sqrt(log(N)/N),
                  solver = "gurobi",
                  progress = FALSE)

# Run dkqs with one worker
plan(multisession, workers = 1)
t1 <- Sys.time()
set.seed(1)
do.call(dkqs, dkqs.args)
t2 <- Sys.time()
time1 <- t2 - t1

# Run dkqs with three workers
plan(multisession, workers = 3)
t1 <- Sys.time()
set.seed(1)
do.call(dkqs, dkqs.args)
t2 <- Sys.time()
time3 <- t2 - t1

# Print the time used
print(sprintf("Time used with 1 core: %s", time1))
print(sprintf("Time used with 3 cores: %s", time3))
```

Note that there are different options available for the `plan` command. The 
availability of the options may depend on the operating systems used. Please 
refer to the instructions from the `future` package for more details.

## Further Examples
For further examples on different procedures in this package, please refer
to the codes in the `example` folder for more details.

## Help, Feature Requests and Bug Reports
Please post an issue on the 
[issues page](https://github.com/conroylau/lpinfer/issues) of the GitHub 
repository. We are happy to help.

## References



 