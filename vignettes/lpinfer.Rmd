---
title: 'lpinfer: An R Package for Inference in Linear Programs'
author: ''
output:
  md_document:
    variant: "markdown_strict"
    preserve_yaml: yes
    toc: yes
    df_print: paged  
  pdf_document:
    toc: yes
    df_print: paged
  github_document:
    keep_html: yes
    toc: yes
    df_print: paged
  html_document:
    keep_md: yes
    toc: yes
    df_print: paged
bibliography: refs.bib
vignette: |
  %\VignetteIndexEntry{dkqs} 
  %\VignetteEncoding{UTF-8} 
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE
)
```

```{r setup, include = FALSE}
require(lpinfer)
require(data.table)
require(pander)
require(gurobi)
require(knitr)
require(kableExtra)
require(foreach)
require(doMC)
require(parallel)
require(PtProcess)
require(doSNOW)
require(doRNG)
require(doParallel)
```

## Introduction
This package provides a set of methods to conduct inference on econometrics 
problems that can be studied by linear programs. Currently, this package 
supports the following tests:

1. Cone-tightening procedure by @dkqs2018
1. Subsampling procedure
1. FSST procedure by @fsst2020

Apart from computing the *p*-values based on the above tests, this package
can also construct confidence intervals and estimate the bounds of the 
estimators in linear programs subject to certain shape constraints.

## Scope of the Vignette
This vignette is intended as a guide to use the `lpinfer` package without
further explanation for the methods. Readers may refer to each of the 
sections for the tests for references to more details about the 
methods.

## Installation and Requirements
`lpinfer` can be installed from our GitHub repository via

```{r eval = FALSE}
devtools::install_github("conroylau/lpinfer")
```

To use most of the functions in this `lpinfer` package, one of the following
packages for solving the linear and quadratic programs is required. There are 
four options for the solver:

1. Gurobi and the R package `gurobi` --- Gurobi can be downloaded from
[Gurobi Optimization](https://www.gurobi.com/). A Gurobi software license is 
required. The license can be obtained at no cost for academic researchers. The 
instructions for installing `gurobi` on R can be found 
[here](https://cran.r-project.org/web/packages/prioritizr/vignettes/gurobi_installation.html#r-package-installation).

1. IBM ILOG CPLEX Optimization Studio (CPLEX) and one of the R packages below 
--- CPLEX can be downloaded from 
[IBM](https://www.ibm.com/analytics/cplex-optimizer). A CPLEX software license
is required, which can be obtained at no cost for academic researchers. There 
are two open-source and free R packages that uses CPLEX, and users are free to
choose one of them. In addition, both packages have to be installed on the 
command line to link the package to the correct CPLEX library. The two packages' 
name and installation instructions are as follows:
   i.  `Rcplex`  --- the instructions to install the R package can be found
   [here](https://cran.r-project.org/web/packages/Rcplex/INSTALL).
   
   i. `cplexAPI` --- the instructions to install the R package can be found
   [here](https://cran.r-project.org/web/packages/cplexAPI/INSTALL).

1. `limSolve` --- a free and open-source package available on CRAN. This can be
installed directly via the `install.packages` command in R.

If no package is specified, one of the above packages will be automatically 
chosen from those that are available.

The package `lpSolveAPI` is only supported in the function `estbounds`
when the L1-norm is used. This is a free and open-source package available on
CRAN. This can be installed directly via the `install.packages` command in R.

## Example {#data}
The classical missing data problem due to @manski1989 is used as a running
example throughout this vignette to demonstrate the commands in this 
`lpinfer` package. We consider this problem here because the sharp bounds for 
the identified set of the expected values can be constructed by linear programs.


In this package, a sample simulated data set on the missing data problem is
included. This can be obtained by `sampledata`. This data set contains 1,000
observations with 2 columns. The following shows the first 10 observations of 
the simulated data set:

```{r, drawData-df}
library(lpinfer)
knitr::kable(head(sampledata, n = 10))
```
where

* `Y` is a multivariate discrete outcome variable that takes value from 0 to 1 
with increment 0.1.
* `D` is a binary treatment variable where *Y*~*i*~ is observed for 
*D*~*i*~ = 1 and not observed for *D*~*i*~ = 0.

## General Syntax

In general, each of the tests in the `lpinfer` requires a data set, a 
`lpmodel` object and some tuning parameters. The tuning parameters are 
different for each of the test and will be explained in later sections. 
For the `lpmodel` object, it consists of five components:

* `A.obs`
* `A.shp`
* `A.tgt`
* `beta.obs`
* `beta.shp`

Here, `A.obs` and `beta.obs` refers to the matrix of coefficients and the RHS
variables for the "observed constraints" in the linear program. `A.shp` and
`beta.shp` refers to the matrix of coefficients and the RHS variables for
the shape constraints. `A.tgt` refers to the matrix of coefficients for the 
linear constraints that we wish to test with `beta.tgt`. Since the object 
`beta.tgt` is a parameter that is being tested, it will be defined outside
the `lpmodel` object. 

Note that not all procedures will require all five components to be present
in the `lpmodel` object. For instance, `A.shp` and `beta.shp` are not required
in the `dkqs` procedure. 

#### Standard form

For consistency, the `lpmodel` assumes that all the above components 
represent linear constraints that are in standard form. Hence, the matrices
in the `lpmodel` object are representing linear equality constraints. 
To impose inequality constraints, users need to first convert them to standard
form by adding appropriate slack and surplus variables. 
To do this, users can use the `standard.form` function in the `lpinfer`
package.

#### Deterministic or stochastic components

Depending on the testing procedure, the five components in the `lpmodel` 
object can be either deterministic or stochastic. The component is said to 
be deterministic if it remains unchanged in the bootstrap procedure. The
component is said to be stochastic if it changes (and depends on the bootstrap 
data) in the bootstrap procedure.

* If the component is deterministic, then the component is typically a 
`matrix` or a `data.frame`. 

* If the component is stochastic, then it is represented by a
`function` or a `list`. If the component is a `function`, then it will be 
re-evaluated based on the bootstrap data in each bootstrap draw. 
If the component is a `list`, then each object in the list represents 
the component in the bootstrap draw.

#### Remarks for the component being a function

If the component is a `function`, then it has to fulfill the following 
requirements:

* The function's only argument is the data set. The function needs to accept
data sets in the class `data.frame`.
* The function can return either one or two objects. If it returns one object, 
then it has to be a vector or a column matrix that represents the estimator for
`beta.obs`. If it has two objects, then it returns the estimator
for `beta.obs` and a square matrix that refers to the estimator of the 
asymptotic variance of `beta.obs`.

#### Example {#eg_fullinfo}

The following is an example on how to construct each of the components in the
`lpmodel` object and how to assign the `lpmodel` object based on the example 
data mentioned [here](#data):


```{r, syntax_fullinfo, eval = TRUE}
# Extract relevant information from data
N <- nrow(sampledata)
J <- length(unique(sampledata[,"Y"])) - 1
J1 <- J + 1

# Construct A.obs
Aobs.full <- cbind(matrix(rep(0, J1 * J1), nrow = J1), diag(1, J1))

# Construct A.tgt
yp <- seq(0, 1, 1/J)
Atgt <- matrix(c(yp, yp), nrow = 1)

# Construct A.shp
Ashp <- matrix(rep(1, ncol(Aobs.full)), nrow = 1)

# Construct beta.obs (a function)
betaobs.fullinfo <- function(data){
  beta <- NULL
  y.list <- sort(unique(data[,"Y"]))
  n <- dim(data)[1]
  yn <- length(y.list)
  for (i in 1:yn) {
    beta.i <- sum((data[,"Y"] == y.list[i]) * (data[,"D"] == 1))/n
    beta <- c(beta, c(beta.i))
  }
  beta <- as.matrix(beta)
  return(list(beta = beta,
              var = diag(yn)))
}

# Define the lpmodel object
lpm.full <- lpmodel(A.obs    = Aobs.full,
                    A.tgt    = Atgt,
                    A.shp    = Ashp,
                    beta.obs = betaobs.fullinfo,
                    beta.shp = c(1))

```

In the above, the `betaobs.fullinfo` function returns two information: `beta`
is a vector of length that is equal to the number of distinct observations 
for *Y* where element *i* of the vector refers to the probability that the
corresponding value of *y*~*i*~ is observed. The `var` object is the 
estimator of the asymptotic variance of `beta`, which is assumed to be an
identity matrix here for illustration purpose.

## Tests {#test}

### Test 1: DKQS cone-tightening procedure

The `dkqs` function in this `lpinfer` package is used to conduct the 
cone-tightening procedure that is proposed by @dkqs2018. For details of the
procedure, readers may refer to section 4.2 and the appendix of @dkqs2018. 

#### Syntax
The `dkqs` command has the following syntax:
```{r, syntax_dkqs , eval = FALSE}
dkqs(data = sampledata,
     lpmodel = lpm.full,
     beta.tgt = .375,
     R = 100,
     tau = sqrt(log(N)/N),
     solver = "gurobi",
     cores = 1,
     progress = FALSE)
```

where

* `data` refers to the data set.
* `lpmodel` refers to the `lpmodel` object.
* `beta.tgt` refers to the parameter that is being tested.
* `R` refers to the total number of bootstraps.
* `tau` refers to the tuning parameter tau. This will be explained 
[here](#tau_dkqs). It can be a scalar or a vector.
* `cores` refers to the number of cores to be used in the parallelized for-loop
for computing the bootstrap test statistics. See [here](#parallel-dkqs) for
more details.
* `progress` refers to the boolean variable for whether the progress bar
should be printed in the testing procedure.

#### Choosing the tau parameter {#tau_dkqs}

Depending on the value of `tau`, the main quadratic program in @dkqs2018 is 
not always feasible. The details on how to choose the maximum tau such 
that the `dkqs` procedure is feasible can be found in the supplemental
appendix of @kamat2019.

In this module, we follow the procedure by @kamat2019 to pick the largest 
feasible `tau`. If the value of `tau` chosen by the user is infeasible, then
the module will evaluate the procedure using the largest possible `tau`. 
Otherwise, the module will evaluate the procedure with the given `tau`.

If `tau` is not specified by the user, the procedure will directly use
the maximum feasible `tau`.

On the other hand, this package provides the flexibility to the users to 
pass multiple tuning parameters at the same time. Hence, `tau` can be a 
vector as well.

#### Components in `lpmodel`

The following table summarizes whether the components in `lpmodel`
can be deterministic or stochastic:

| Component in `lpmodel`| Property |
| --------------- | :-------------- |
|`A.obs`| Deterministic |
|`A.shp`| Not used |
|`A.tgt`| Deterministic |
|`beta.obs`| Stochastic |
|`beta.shp`| Not used |

`A.shp` and `beta.shp` will be ignored in the `dkqs` procedure.

#### Data structure

The function defined in this function needs to generate an estimator
for the `beta.obs` object. 

In addition, this procedure has an additional requirement on the data 
structure: the column that represents the outcome variable has to be called
either "Y" or "y".

#### Example

The following is what happens when the above code is run:
``` {r, dkqs_eg1, eval = TRUE}
set.seed(1)
dkqs.full1 <- dkqs(data = sampledata,
                   lpmodel = lpm.full,
                   beta.tgt = .375,
                   R = 100,
                   tau = sqrt(log(N)/N),
                   solver = "gurobi",
                   cores = 1,
                   progress = FALSE)
print(dkqs.full1)
```

As noted in the syntax section, we provide the flexibility for users to conduct
inference with multiple tuning parameters at the same time. The following is
an example when the user specifies multiple taus in the `dkqs` procedure:
``` {r, dkqs_eg2, eval = TRUE}
set.seed(1)
dkqs.full2 <- dkqs(data = sampledata,
                   lpmodel = lpm.full,
                   beta.tgt = .375,
                   R = 100,
                   tau = c(.1, .2, .3, .5),
                   solver = "gurobi",
                   cores = 1,
                   progress = FALSE)
print(dkqs.full2)
```

Users can get a more detailed summary of the results when applying the 
`summary` command on the resulting object:
``` {r, dkqs_eg2summary, eval = TRUE}
summary(dkqs.full2)
```

#### Alternative approach
The approach that has been demonstrated so far is known as the 
full information approach. The inference procedures in this `lpinfer` package
is not limited to any single approach. In this section, we demonstrate 
how to use the two moments approach. This alternative method is known as the
two moments approach because the two moments **E**[*Y*~*i*~] and 
**E**[*Y*~*i*~*D*~*i*~] are used in the inference procedure.

Similar to what has been demonstrated in the [example](#eg_fullinfo) earlier,
some of the components has to be updated in the `lpmodel` object. 
This can be done as follows:

```{r, syntax_twom, eval = TRUE}
# Construct A.obs
Aobs.twom <- matrix(c(rep(0,J1), yp, rep(0,J1), rep(1, J1)), nrow = 2,
                     byrow = TRUE)

# Construct beta.obs (a function)
betaobs.twom <- function(data){
  beta <- matrix(c(0,0), nrow = 2)
  n <- dim(data)[1]
  beta[1] <- sum(data[,"Y"] * data[,"D"])/n
  beta[2] <- sum(data[,"D"])/n
  return(list(beta = beta,
              var = diag(2)))
}

# Define the lpmodel object
lpm.twom <- lpmodel(A.obs    = Aobs.twom,
                    A.tgt    = Atgt,
                    A.shp    = Ashp,
                    beta.obs = betaobs.twom,
                    beta.shp = c(1))

```

The `dkqs` procedure can be ran in the same fashion as before. The following
is an example on how this can be ran:

``` {r, dkqs_eg3, eval = TRUE}
set.seed(1)
dkqs.full3 <- dkqs(data = sampledata,
                   lpmodel = lpm.twom,
                   beta.tgt = .375,
                   R = 100,
                   tau =  sqrt(log(N)/N),
                   solver = "gurobi",
                   cores = 1,
                   progress = FALSE)
print(dkqs.full3)
```

As shown above, we get the same *p*-value as before.

### Test 2: Subsampling procedure

The `subsample` function in this package carries out the test using 
the subsampling procedure. 

#### Syntax

The `subsample` command has the following syntax:
```{r, subsample_syntax, eval = FALSE}
subsample(data = sampledata, 
          lpmodel = lpm.twom,
          beta.tgt = 0.375,
          R = 100,
          solver = "gurobi",
          cores = 1,
          norm = 2,
          phi = 2/3,
          alpha = 0.05,
          progress = FALSE)
```
where 

* `phi` refers to the parameter that controls the size of each subsample. 
This will be further explained [here](#phi_subsample).
* `norm` refers to the norm used in the objective function.

The rest of the arguments are the same as that in the `dkqs` procedure.

#### Choosing the phi parameter {#phi_subsample}

The `phi` parameter is a parameter between 0 and 1. The sample size
of the original data to the power `phi` is the size of each 
subsample. Unlike the other bootstrap procedures, the bootstrap data are
drawn without replacement.

#### Components in `lpmodel`

The following table summarizes whether the components in `lpmodel`
can be deterministic or stochastic:

| Component in `lpmodel`| Property |
| --------------- | :-------------- |
|`A.obs`| Deterministic or stochastic |
|`A.shp`| Deterministic or stochastic |
|`A.tgt`| Deterministic or stochastic |
|`beta.obs`| Stochastic |
|`beta.shp`| Deterministic or stochastic |

#### Example

The following is what happens when the above code is run:
``` {r, subsample_eg1, eval = TRUE}
set.seed(1)
subsample.full <- subsample(data = sampledata, 
                            lpmodel = lpm.full,
                            beta.tgt = 0.375,
                            R = 100,
                            solver = "gurobi",
                            cores = 1,
                            norm = 2,
                            phi = 2/3,
                            alpha = 0.05,
                            progress = FALSE)
print(subsample.full)
```

Again, more detailed information can be extracted via the `summary` command:
``` {r, subsample_eg2, eval = TRUE}
summary(subsample.full)
```


### Test 3: FSST procedure

The `fsst` function in this `lpinfer` package is used to conduct the 
testing procedure by @fsst2020.

#### Syntax

The `fsst` command has the following syntax:
```{r, fsst_syntax, eval = FALSE}
fsst(data = sampledata, 
     lpmodel = lpm.full,
     beta.tgt = 0.375,
     R = 100,
     lambda = 0.5,
     rho = 1e-4,
     n = nrow(sampledata),
     weight.matrix = "diag",
     solver = "gurobi",
     cores = 1,
     progress = FALSE)
```
where 

* `lambda` refers to the tuning parameter that affects test statistics in the
bootstrap cone components. Users can pass multiple `lambda`s in this 
argument.
* `rho` refers to the parameter used to studentize the variance matrices
in the FSST procedure.
* `n` is optional if `data` is passed. `n` is a variable that refers to the
number of rows of `data`. If the `beta.obs` in `lpmodel` is a `list`, then
users can skip `data` and pass the number of rows of the `data` as `n` instead.
* `weight.matrix` is a string that determines the weighting matrix. 
The details can be found [here](#weight_matrix).

The rest of the arguments are the same as that in the `dkqs` procedure.

#### Weighting matrix

This procedure provides three options for the weighting matrix in the FSST
procedure:

|`weight.matrix` | Definition |
| --------------- | :-------------- |
|`identity`| Identity matrix|
|`avar`| Inverse of the asymptotic variance of `beta.obs`|
|`diag`| Diagonal matrix with elements equal to the diagonal entries of `avar`|

#### Components in `lpmodel`

The following table summarizes whether the components in `lpmodel`
can be deterministic or stochastic:

| Component in `lpmodel`| Property |
| --------------- | :-------------- |
|`A.obs`| Deterministic |
|`A.shp`| Deterministic |
|`A.tgt`| Deterministic |
|`beta.obs`| Stochastic |
|`beta.shp`| Deterministic |


#### Example

The following is what happens when the above code is run:
``` {r, fsst_eg1, eval = TRUE}
set.seed(1)
fsst.full1 <- fsst(data = sampledata, 
                   lpmodel = lpm.full,
                   beta.tgt = 0.375,
                   R = 100,
                   lambda = 0.5,
                   rho = 1e-4,
                   n = nrow(sampledata),
                   weight.matrix = "diag",
                   solver = "gurobi",
                   cores = 1,
                   progress = FALSE)
print(fsst.full1)
```

As noted above, the `fsst` procedure provides the flexibility for users to 
pass multiple tuning parameters.

``` {r, fsst_eg2, eval = TRUE}
set.seed(1)
fsst.full2 <- fsst(data = sampledata, 
                   lpmodel = lpm.full,
                   beta.tgt = 0.375,
                   R = 100,
                   lambda = c(0.1, 0.2, 0.5),
                   rho = 1e-4,
                   n = nrow(sampledata),
                   weight.matrix = "diag",
                   solver = "gurobi",
                   cores = 1,
                   progress = FALSE)
print(fsst.full2)
```


Again, more detailed information can be extracted via the `summary` command:
``` {r, fsst_eg2summary, eval = TRUE}
summary(fsst.full2)
```


## Constructing Bounds subject to Shape Constraints

Another key application of the `lpinfer` package is to construct the 
bounds estimates subject to certain shape constraints. This is obtained by the
`estbounds` function. The linear program for obtaining the exact bounds 
subject to shape constraints may not be necessarily feasible. Hence, an 
`estimate` option is available. The estimation can be conducted via L1-norm 
or L2-norm.

#### Syntax

``` {r, estbounds_syntax, eval = FALSE}
estbounds(data = sampledata,
          lpmodel = lpm.full,
          kappa = 1e-5,
          norm = 2,
          solver = "gurobi",
          estimate = FALSE,
          progress = FALSE)
```
where 

* `norm` refers to the norm used in the optimization problem. The norms
that are supported by this function are L1-norm and L2-norm. See 
[here](#estbounds_norm) for more details.
* `kappa`  refers to the parameter used in the second step of the two-step
procedure for obtaining the solution subject to the shape constraints.
* `estimate` refers to the boolean variable that indicate whether the estimated
problem should be considered.

The rest of the arguments are the same as that in the `dkqs` procedure.

#### Norms {#estbounds_norm}
In constructing the estimated bounds, users are free to choose the L1-norm or
the L2-norm. For the estimation with L2-norm, users need to choose `gurobi` as 
the solver. For the estimation with L1-norm, users can choose one of the 
following packages as the solver:

* `gurobi`,
* `limSolve`,
* `cplexAPI`,
* `Rcplex`,
* `lpSolveAPI`.


#### Example

The following is what happens when the above code is run:
``` {r, estbounds_eg1, eval = TRUE}
set.seed(1)
estbounds.full <- estbounds(data = sampledata,
                            lpmodel = lpm.full,
                            kappa = 1e-5,
                            norm = 2,
                            solver = "gurobi",
                            estimate = TRUE,
                            progress = FALSE)
print(estbounds.full)
```

Again, more detailed information can be extracted via the `summary` command:
``` {r, estbounds_eg1summary, eval = TRUE}
summary(estbounds.full)
```


## Constructing Confidence Intervals

Apart from conducting inference and estimating the bounds, the `lpinfer` 
package can also construct confidence intervals via the `invertci` function. 
The confidence interval is constructed by evaluating the *p*-value of a test 
and applying the bisection method.

#### Syntax {#invertci_syntax}
The syntax of the `invertci` function is as follows:

```{r, invertci_syntax, eval = FALSE}
invertci(f = subsample, 
         farg = subsample.args, 
         alpha = 0.05, 
         lb0 = NULL, 
         lb1 = NULL, 
         ub0 = NULL, 
         ub1 = NULL, 
         tol = 0.0001, 
         max.iter = 20, 
         df_ci = NULL,
         progress = FALSE)
```
where 

* `f` refers to the function that represents a testing procedure.
* `farg` refers to the list of arguments to be passed to the function of 
testing procedure. The details can be found [here](#argument_invertci).
* `alpha` refers to the significance level of the test. Please refer to the 
details [here](#multiple_ci).
* `lb0` refers to the logical lower bound for the confidence interval.
* `lb1` refers to the maximum possible lower bound for the confidence interval.
* `ub0` refers to the logical upper bound for the confidence interval.
* `ub1` refers to the minimum possible upper bound for the confidence interval.
* `tol` refers to the tolerance level in the bisection method.
* `max.iter` refers to the maximum number of iterations in the bisection 
method.
* `df_ci` refers to `data.frame` that consists of the points and the 
corresponding *p*-values that have been tested in constructing the confidence 
intervals. The details can be found [here](#dfci_invertci).
* `progress` refers to the boolean variable for whether the result messages 
should be displayed in the procedure of constructing confidence interval. 

#### Specifying the argument {#argument_invertci}
To use the `invertci` function, the arguments for the test statistic has to
be specified and passed to the `farg` argument. For instance, if the testing
procedure `subsample` is used, the arguments can be defined as follows:

```{r, invertci.args, eval = TRUE}
subsample.args <- list(data = sampledata,
                       lpmodel = lpm.full,
                       R = 100,
                       phi = .75,
                       solver = "gurobi",
                       cores = 1,
                       progress = FALSE)
```

Note that the argument for the target value of beta, i.e. the value to be 
tested under the null, is not required in the above argument assignment.

#### Specifying the Data Frame `df_ci` {#dfci_invertci}
If the *p*-values at certain points have already been evaluated, users can
store them in a data frame and pass it to the function `invertci`. The 
requirement for the data frame is as follows:

* The data frame can only has two columns. The first column is `point` (which
contains the values of betas that has been evaluated) and the second column is
`value` (which corresponds to the *p*-values being evaluated).
* The data frame can only contain numeric values.

#### Example

The following shows a sample output of the function `invertci` that is used to 
the confidence interval for the test `dkqs` with significance level 0.05.

```{r, eval = TRUE}
set.seed(1)
invertci.subsample1 <- invertci(f = subsample,
                                farg = subsample.args, 
                                alpha = 0.05, 
                                lb0 = 0, 
                                lb1 = 0.4, 
                                ub0 = 1, 
                                ub1 = 0.6, 
                                tol = 0.001, 
                                max.iter = 5, 
                                df_ci = NULL, 
                                progress = FALSE)
print(invertci.subsample1)
```

The details for each iteration can be obtained in real-time by setting
`progress` as `TRUE` or applying the `summary` command on the resulting
object:
``` {r, eval = TRUE}
summary(invertci.subsample1)
```

#### Constructing Multiple Confidence Intervals {#multiple_ci}

The function `invertci` can also be used to generate multiple confidence
intervals if the argument `alpha` is a vector. For instance, the following
code produces confidence intervals for alpha equals 0.01, 0.05 and 0.1.

```{r, eval = TRUE, warning = FALSE}
set.seed(1)
invertci.subsample2 <- invertci(f = subsample, 
                                farg = subsample.args, 
                                alpha = c(0.01, 0.05, 0.1), 
                                lb0 = 0, 
                                lb1 = 0.4, 
                                ub0 = 1, 
                                ub1 = 0.6, 
                                tol = 0.001, 
                                max.iter = 5, 
                                df_ci = NULL, 
                                progress = FALSE)
print(invertci.subsample2)
```

Again, the detailed steps in constructing the confidence intervals can be 
obtained as follows:
``` {r, eval = TRUE}
summary(invertci.subsample2)
```

Since the details of the iterations could potentially be a long list of
outputs, users may specify a particular list of output that they wish to
read in the `summary` command. For instance, to only print the details of the 
iterations when the significance level is 0.05, it can be done as follows:
``` {r, eval = TRUE}
summary(invertci.subsample2, alphas = 0.05)
```

## Parallel Programming {#parallel-dkqs}

The `lpinfer` package supports the use of parallel programming in 
computing the bootstrap test statistics to reduce the computational time. 
To use parallel programming, specify the number of cores that you would like 
to use in the argument `cores`. If you do not want to use parallel programming, 
you may input 1 or any other non-numeric variables in `cores`.

For best performance, it is advisable to specify the number of cores to be 
less than or equal to the cores that you have on your machine.

The computational time for using multiple cores should be
shorter than using a single core for a large number of bootstraps. This is
illustrated by the example via the `dkqs` procedure below:

```{r, eg.cores, eval = TRUE}
dkqs.args <- list(data = sampledata,
                  lpmodel = lpm.full,
                  beta.tgt = .375,
                  R = 100,
                  tau = sqrt(log(N)/N),
                  solver = "gurobi",
                  progress = FALSE)

# Run dkqs with one core
dkqs.args$cores <- 1
t10 <- Sys.time()
set.seed(1)
do.call(dkqs, dkqs.args)
t11 <- Sys.time()
time1 <- t11 - t10

# Run dkqs with eight cores
dkqs.args$cores = 8
t80 <- Sys.time()
set.seed(1)
do.call(dkqs, dkqs.args)
t81 <- Sys.time()
time8 <- t81 - t80

# Print the time used
print(sprintf("Time used with 1 core: %s", time1))
print(sprintf("Time used with 8 cores: %s", time8))
```

## Help, Feature Requests and Bug Reports
Please post an issue on the 
[GitHub repository](https://github.com/conroylau/lpinfer/issues). We 
are happy to help.

## References



 